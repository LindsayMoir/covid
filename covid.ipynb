{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid-19, Time To Get Out?\n",
    "\n",
    "![alt text](man_window_covid.jpg \"Title\")\n",
    "\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#contact\">Contact Information</a></li> \n",
    "<p>\n",
    "<li><a href=\"#business\">1. Business Understanding</a></li> \n",
    "<li style=\"margin-left: 40px\"><a href=\"#q1\">Q1: Who are the best and worst countries dealing with Covid-19?</a></li> \n",
    "<li style=\"margin-left: 40px\"><a href=\"#q2\">Q2: Which countries have flattened or are flattening the curve?</a></li>\n",
    "<li style=\"margin-left: 40px\"><a href=\"#q3\">Q3: Can I see a global geographic representation of infections?</a></li>\n",
    "<li style=\"margin-left: 40px\"><a href=\"#q4\">Q4: What is the projected global mortality by December 31, 2020?</a></li> \n",
    "<li style=\"margin-left: 40px\"><a href=\"#q5\">Q5: Number of unnecessary deaths in the 3 most poorly managed countries?</a></li> \n",
    "<p>\n",
    "<li><a href=\"#data\">2. Data Understanding</a></li> \n",
    "<li style=\"margin-left: 40px\"><a href=\"#gather\">Gather</a></li>\n",
    "<li style=\"margin-left: 40px\"><a href=\"#describe\">Describe, Clean, Explore, and Verify</a></li>\n",
    "<p>\n",
    "<li><a href=\"#data_prep\">3. Data Preparation</a></li> \n",
    "<li style=\"margin-left: 40px\"><a href=\"#a1\">Answer To Q1</a></li>    \n",
    "<li style=\"margin-left: 40px\"><a href=\"#a2\">Answer To Q2</a></li>    \n",
    "<li style=\"margin-left: 40px\"><a href=\"#a3\">Answer To Q3</a></li> \n",
    "<li style=\"margin-left: 40px\"><a href=\"#a4\">Answer To Q4</a></li>\n",
    "<li style=\"margin-left: 40px\"><a href=\"#a5\">Answer To Q5</a></li>\n",
    "<p>\n",
    "<li><a href=\"#model\">4. Model Data</a></li>\n",
    "<li style=\"margin-left: 40px\"><a href=\"#a4\">Answer To Q4</a></li>\n",
    "<p>\n",
    "<li><a href=\"#conclusions\">5. Results and Conclusions</a></li>\n",
    "<li style=\"margin-left: 40px\"><a href=\"#limitations\">Limitations</a></li>\n",
    "<p>\n",
    "<li><a href=\"#deploy\">6. Deploy</a></li>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "# Regular libraries\n",
    "from datetime import datetime, timedelta\n",
    "import dateutil.parser\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import Image\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams['figure.figsize'] = (12,8)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "# ARIMA libraries\n",
    "from math import sqrt\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.arima_model import ARIMAResults\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# There are a lot of warnings. Lets just ignore these.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA', FutureWarning)\n",
    "\n",
    "# Global map visualization\n",
    "import plotly.express as px\n",
    "\n",
    "# Parallel Libraries\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='contact'></a>\n",
    "## Contact Information\n",
    "\n",
    "> - Lindsay Moir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='business'></a>\n",
    "# Business Understanding\n",
    "\n",
    "> Covid-19 is a global pandemic. We have over 150 countries affected and many approaches to the pandemic. It is possible to use data science to evaluate the leadership of countries by using mortality as a litmus test. This notebook, collects the data necessary for that, graphically shows the results for those countries, and then also runs a machine learning model to predict the total mortality by the end of 2020 (global not by country). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q1'></a>\n",
    "### Question 1\n",
    "\n",
    "#### Who are the best and worst countries in terms of per million population and wealth dealing with Covid-19?\n",
    "\n",
    "We will  use people per million and median income as our metrics for this. An extremely small country would have a relatively small number of deaths and could be totally incompetent at managing covid. Whereas a large country (by population) could do a great job and still have a large number of deaths. Also we have included the Transparency International ranking. Many countries are manipulating their data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q2'></a>\n",
    "### Question 2\n",
    "\n",
    "#### Which countries have flattened or are flattening the curve?\n",
    "\n",
    "This will be defined as the number of New_Cases per day over the last 14 days. We take the last date that data is available and we see if the number of New_Cases is going down. We will do this with a scatter plot and a regression line that is fitted to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q3'></a>\n",
    "### Question 3\n",
    "\n",
    "#### Can I see a global geographic representation of infections?\n",
    "\n",
    "We want to see the data on a global map. This allows us to quickly see where the hot spots are. We also have additional features that we will be visualizing on these maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q4'></a>\n",
    "### Question 4\n",
    "\n",
    "#### What is the projected global mortality as of December 31, 2020?\n",
    "\n",
    "We will use the Auto Regressive Integrated Moving Average (ARIMA) technology for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q5'></a>\n",
    "### Question 5\n",
    "\n",
    "#### Number of unnecessary deaths in the 3 most poorly managed countries?\n",
    "\n",
    "What is the real world effect of political leadership on the 3 most poorly managed countries? What number of deaths can we reasonably assign to political leadership failures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gather'></a>\n",
    "### Gather\n",
    "\n",
    "- I am using data from John Hopkins University. There is a GitHub repository that holds this data.  To refresh it just go to https://github.com/CSSEGISandData/COVID-19 and hit the Clone of download button. It will ask to launch GitHub Desktop and will update it automatically.\n",
    "- The data for median income and population comes from https://worldpopulationreview.com/countries/median-income-by-country/\n",
    "- World Bank Country Codes comes from https://wits.worldbank.org/wits/wits/witshelp/content/codes/country_codes.htm\n",
    "- The Transparency International Dataset comes from https://www.transparency.org/cpi2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### John Hopkins Covid-19 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns change over time. I need to figure out what the super set of these column names are. Use glob and sets and a loop to create this result. I am also switching machines and the file paths are slightly different due to the fact that they are named differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'D:\\GitHub\\COVID-19\\csse_covid_19_data\\csse_covid_19_daily_reports'\n",
    "all_files = glob.glob(path + r\"\\*.csv\")\n",
    "\n",
    "li_set = {}\n",
    "\n",
    "for filename in all_files:\n",
    "    \n",
    "    # Just get the first few rows and the column names\n",
    "    df = pd.read_csv(filename, nrows=1)\n",
    "    cols = df.columns\n",
    "    cols = set(cols)\n",
    "    li_set = cols.union(li_set)\n",
    "\n",
    "print(li_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the LAST dfs column names\n",
    "all_df_cols = df.columns\n",
    "all_df_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "li_set is the superset of all column names. So, it is NOT the same as the last csv that I have. I have to do some renaming with every csv that comes in. Now that we know that, we know what column names to change and now we can bring in the entire set of files and concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_df(df, temp_df):\n",
    "    \"\"\"Takes the columns from the 'short df' and puts them in a temp df.\n",
    "    This temp_df will later be put into a df with other columns.\"\"\"\n",
    "    \n",
    "    # Pandas really does not like '/' or ' ' in a column name so ... time to rename\n",
    "    df.rename(columns={'Province/State': 'Province_State', \n",
    "                       'Country/Region': 'Country_Region',\n",
    "                       'Last Update': 'Last_Update',\n",
    "                       'Latitude': 'Lat',\n",
    "                       'Longitude': 'Long_'}, inplace=True)\n",
    "    \n",
    "    # Get the current_df columns\n",
    "    cols = df.columns\n",
    "    \n",
    "    # For loop for putting the appropriate columns in temp_df\n",
    "    for col in cols:\n",
    "        temp_df[col] = df[col]\n",
    "        \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    \n",
    "    # Read in the next csv\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Get the date of the file\n",
    "    date_string = filename[-14:-4]\n",
    "    \n",
    "    # Insert the date_string into the first column of the df\n",
    "    df.insert(0, \"Date_\", date_string) \n",
    "    \n",
    "    # If it is a full df then just append it\n",
    "    if df.shape[1] == 13:\n",
    "        li.append(df)\n",
    "    \n",
    "    else:\n",
    "        # Need to build a df that is the same number of columns as the largest df (last one)\n",
    "        # Create a temp_df to hold everything\n",
    "        temp_df = pd.DataFrame(data=np.nan, columns=all_df_cols.insert(0, 'Date_'), index=df.index)\n",
    "        \n",
    "        # Call function to create the short_df\n",
    "        df = short_df(df, temp_df)\n",
    "\n",
    "        # append df to li\n",
    "        li.append(df)\n",
    "\n",
    "all_df = pd.concat(li)\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date_ column to date\n",
    "all_df['Date_'] =  pd.to_datetime(all_df['Date_'], infer_datetime_format=True)\n",
    "\n",
    "# Sort based on Date_ then Country_Region\n",
    "all_df = all_df.sort_values(['Date_', 'Country_Region'])\n",
    "\n",
    "# reset the index\n",
    "all_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the date of the last row in the file to establish the currency of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The \"data date\" is {all_df[\"Date_\"].iloc[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Population and MedianPerCapitaIncome\n",
    "\n",
    "Now we need population and medium income data. This has been placed on disk in the local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in population and income data\n",
    "pop_med_income_df = pd.read_csv(r'data/pop_med_income.csv')\n",
    "pop_med_income_df.rename({\"MedianPerCapitaIncome\": 'MPC_Inc'}, axis=1, inplace=True)\n",
    "pop_med_income_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_med_income_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Country Codes\n",
    "\n",
    "I had an already existing data file that I had originally gotten from World Bank https://wits.worldbank.org/wits/wits/witshelp/content/codes/country_codes.htm. I have added items to it since, the names of the countries keep on changing in the different datasets that I encounter. Eventually this should be bullet proof. Some data munging was required on this. The John Hopkins data does not have an ISO standard Country column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to add a country code\n",
    "country_codes_df = pd.read_csv(r'data/country_codes_edited.csv')\n",
    "country_codes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets rename the 3 letter code column\n",
    "country_codes_df.rename({'ISO3166-1-Alpha-3': 'Alpha_3'}, axis=1, inplace =True)\n",
    "country_codes_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want Country_Region and Alpha-3\n",
    "country_codes_df = country_codes_df[['Country_Region', 'Alpha_3']]\n",
    "country_codes_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transparency International\n",
    "\n",
    "One of the huge issues with the covid-19 data is there are lots of politics being played with the numbers by politicians. We will use the Transparency International https://www.transparency.org/cpi2019 data 2019 to give us an inkling of whether or not we can trust the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_int_df = pd.read_csv(r'data/transparency_international.csv')\n",
    "trans_int_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dfs (country_codes_df and pop_med_income_df)\n",
    "pop_inc_cc_codes = country_codes_df.merge(pop_med_income_df)\n",
    "pop_inc_cc_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dfs (country_codes_df and pop_med_income_df)\n",
    "pop_inc_cc_codes = pop_inc_cc_codes.merge(trans_int_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_inc_cc_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will merge this dataframe with the all_df after we have done some aggregations with all_df. However, right now we need to go to the next step which is to describe the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='describe'></a>\n",
    "### Describe, Clean, Explore, and Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are all of the numeric columns. For our analysis we can drop FIPS, Recovered, and Active. We will keep the balance of the numeric columns. We have put a record of this in the todos in the Clean section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "\n",
    "Check for duplicates now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "nof_duplicates = all_df.duplicated().sum()\n",
    "if nof_duplicates < 5:\n",
    "    all_df.drop_duplicates(inplace=True)\n",
    "    print(f'There is/are {nof_duplicates}, which has/have been dropped.')\n",
    "else:\n",
    "    print(f'We have a large number of duplicates ({nof_duplicates}). We are exiting the program.')\n",
    "    sys.exit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent no duplicates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "\n",
    "We will also drop Admin2, Province_State, Last_Update, and Combined_Key. We are doing everything with Country. We have put a record of this in the todos in the Clean section. However, in order to use our time efficiently as we are describing the data, we will drop these columns now. There is no sense describing columns that you are dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code\n",
    "\n",
    "# Make a copy to avoid the inevitable Pandas warnings.\n",
    "slim_df = all_df.copy(deep=True)\n",
    "\n",
    "# Just keep the columns we want.\n",
    "slim_df = slim_df[['Date_', 'Country_Region', 'Lat', 'Long_', 'Confirmed', 'Deaths']]\n",
    "\n",
    "# Test\n",
    "slim_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null status\n",
    "slim_df.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define\n",
    "\n",
    "Add fill backwards to the clean section. There are a few missing values. Need to deal with this now so that we can look at data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code\n",
    "slim_df = slim_df.bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# Null status\n",
    "slim_df.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent no nulls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can tell from the .describe() method this is essentially a dataframe of outliers. We are just trying to see the spread, so ... we can be creative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slim_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numeric columns\n",
    "cols = slim_df.select_dtypes('float').columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all of the rows that are all zeros\n",
    "no_zeros_df = slim_df[(slim_df[cols].T != 0).all()]\n",
    "print(no_zeros_df.shape)\n",
    "no_zeros_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run some boxplots (below) since we will not have divide by 0 errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_zeros_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "no_zeros_df.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a log10 of Confirmed and Deaths\n",
    "log10_df = no_zeros_df[cols]\n",
    "log10_df = np.log10(log10_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot Confirmed\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=log10_df['Confirmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot Deaths\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(x=log10_df['Deaths'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, no real insightful information to be gained from this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of Cleaning Issues\n",
    "\n",
    "all_df\n",
    "- We will need several versions of dataframes. Each question requires slightly different data to drive the result.\n",
    "- There are nulls in all_df that we have filled backwards. The reason that we fill backwards is that we think that the reporting is slow. This is a time series. Filling backwards is close to the best value that we could get if not the best.\n",
    "\n",
    "country_codes_df\n",
    "- This has been data wrangled to successfully merge with all_df on the Country_Region column. \n",
    "\n",
    "pop_med_income_df\n",
    "- This was data wrangled and should yield no issues while merging with all_df on the Country_Region column. Thereafter we will use the Alpha_3 column.\n",
    "\n",
    "trans_int_df\n",
    "- This dataset is clean and does not require any wrangling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_prep'></a>\n",
    "# Data Preparation\n",
    "\n",
    "As we go thru this notebook we will be transforming the data so that we can answer the questions posed. To start lets look at some global visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confirmed_death(df, place):\n",
    "    \"\"\"Plots daily running total of confirmed cases and deaths\"\"\"\n",
    "\n",
    "    start = df.index[0].strftime('%Y-%m-%d')\n",
    "    end = df.index[-1].strftime('%Y-%m-%d')\n",
    "    title = ('From {} to {} Daily Running Total Statistics For Covid-19 ({})').format(start, end, place)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('# Of Cases and Deaths Per Day')\n",
    "    plt.plot('Confirmed', data=df, color='orange', linewidth=2, label='Confirmed Cases')\n",
    "    plt.plot('Deaths', data=df, linewidth=2, label='Deaths')\n",
    "    plt.legend()\n",
    "    plt.savefig(r'pics_final/running_totals.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mortality_rate(df, place):\n",
    "    \"\"\"Mortality rate plotted\"\"\"\n",
    "\n",
    "    start = df.index[0].strftime('%Y-%m-%d')\n",
    "    end = df.index[-1].strftime('%Y-%m-%d')\n",
    "    title = ('From {} to {} Change in Mortality For Covid-19 ({})').format(start, end, place)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Mortality Rate')\n",
    "    plt.plot('Mortality_Rate', data=df, linewidth=2, label='Deaths')\n",
    "    plt.legend()\n",
    "    plt.savefig(r'pics_final/mortality_rate.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mortality_population(df, sign):\n",
    "    \"\"\"Plot the Deaths_e6. This is adjusted for population. It shows what countries are doing well or poorly\n",
    "    considering their Median Income and Transparency International score\"\"\"\n",
    "    \n",
    "    sns.set_color_codes(\"pastel\")\n",
    "    sns.barplot(x=\"Deaths_e6\", y=\"Country_Region\", data=df,\n",
    "                label=\"Deaths Per Million Population\", palette=\"Blues_d\")\n",
    "\n",
    "    # Title\n",
    "    title = ('Mortality Rate Per Million Population As of {}').format(end)\n",
    "    plt.xlabel('Deaths Per Million')\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Construct legend\n",
    "    legend_ = f'Includes Only Countries With {sign} 50% Median Per Capita Income and >= 60 on Transparency International Score'\n",
    "    plt.legend([legend_])\n",
    "    \n",
    "    # Final housekeeping\n",
    "    sns.despine(left=True, bottom=True)\n",
    "    if sign == '<':\n",
    "        plt.savefig(r'pics_final/honest_but_no_money.png');\n",
    "    else:\n",
    "        plt.savefig(r'pics_final/peer_group.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_new_cases(df, place, days):\n",
    "    \"\"\"Number of New_Cases with Regression Line\n",
    "    returns the slope and intercept of the line\"\"\"\n",
    "    \n",
    "    # Create the title\n",
    "    end = df['Date_'].iloc[-1].strftime('%Y-%m-%d')\n",
    "    title1 = (f'New Cases For Last {days} Days Prior To And Including {end} For Covid-19 With Regression Line ({place}) ')\n",
    "    title2 = (f'\\n{country} Has a Transparency International Score of {recent[\"TI_2019\"].iloc[0]}')\n",
    "    plt.title(title1 + title2)\n",
    "    \n",
    "    # We want the indexes in a list so we can place them on the x axis\n",
    "    new_labels = df['Date_'].dt.strftime('%m-%d').tolist()\n",
    "    plt.xticks(np.arange(len(new_labels)), new_labels)\n",
    "    \n",
    "    # Label the axis\n",
    "    plt.xlabel('Consecutive Days')\n",
    "    plt.ylabel('New_Cases Daily')\n",
    "    \n",
    "    # Create the plot\n",
    "    p = sns.regplot(x=list(range(len(df))), y='New_Cases', data=df)\n",
    "    \n",
    "    # Create legend\n",
    "    plt.legend(['New_Cases'], loc='upper center')\n",
    "    \n",
    "    # Make sure the plot is shown\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(r'pics_final/' + place + '.jpg')\n",
    "\n",
    "    # Calculate the slope and intercept\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x=p.get_lines()[0].get_xdata(),\n",
    "                                                                   y=p.get_lines()[0].get_ydata())\n",
    "    \n",
    "    return slope, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_deaths(df, place):\n",
    "    \"\"\"Number of Deaths with Regression Line\"\"\"\n",
    "    \n",
    "    # Get start and end times of df\n",
    "    start = df.index[0].strftime('%Y-%m-%d')\n",
    "    end = df.index[-1].strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Format the title and labels\n",
    "    title = ('From {} to {} Daily Mortality For Covid-19 ({})').format(start, end, place)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('# Of Deaths Per Day')\n",
    "    \n",
    "    # Plot regression line\n",
    "    sns.regplot(x=range(len(df)), y='Deaths', data=df, label='Deaths')\n",
    "    \n",
    "    # Legend\n",
    "    plt.legend()\n",
    "    \n",
    "    # Save the chart\n",
    "    plt.savefig(r'pics_final/running_totals.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_world_map(q3_df, values):\n",
    "    \"\"\"Maps features onto a global map for visualization purposes\"\"\"\n",
    "    \n",
    "    # unpack values\n",
    "    feature, legend_string, title_string = values\n",
    "    \n",
    "    fig = px.choropleth(q3_df, locations = \"Alpha_3\",\n",
    "                        color = feature, \n",
    "                        hover_name = \"Country_Region\", # column to add to hover information\n",
    "                        color_continuous_scale=px.colors.sequential.Turbo, \n",
    "                        title = title_string)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_deaths(df, place):\n",
    "\n",
    "    # Add a new column that is Deaths_e6 (deaths per million)\n",
    "    forecast_df['Deaths_e6'] = forecast_df['Deaths'] / 1000000\n",
    "\n",
    "    # Assemble title\n",
    "    start = forecast_df.index[0].strftime('%Y-%m-%d')\n",
    "    title = ('{} Forecast Cumulative Deaths In Millions For Covid-19 ({} to {})').format(place, start, forecast_date)\n",
    "    plt.title(title)\n",
    "\n",
    "    # Create x and y axis labels\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cumulative Deaths In Millions')\n",
    "\n",
    "    # Create plot\n",
    "    plt.plot('Deaths_e6', data=forecast_df, linewidth=4, label='Deaths')\n",
    "    plt.legend()\n",
    "    plt.savefig(r'pics_final/' + place + '_prediction.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_angle(df, a):\n",
    "    \"\"\"Calculates the angle of the regression line\"\"\"\n",
    "    \n",
    "    # Getting the length of all 3 sides\n",
    "    # a is visualized based on a regression line of April 30 2020. This may change (obviously)\n",
    "    b = (df.index[-1] - df.index[0]).days # Length of the dataframe (x axis).\n",
    "    c = math.sqrt(a**2 + b**2)  # calculate the hypotenuse\n",
    "    \n",
    "    # Calculate the tangent\n",
    "    tan = math.atan(a/b)\n",
    "    angle = math.degrees(tan)\n",
    "    \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_deaths(angle, start, end):\n",
    "    \"\"\"Calculates the number of deaths based on the angle generated from calc_angle and days calculated from dates\"\"\"\n",
    "    \n",
    "    # Calculate the number of days for b (x axis / adjacent)\n",
    "    start = dateutil.parser.parse(start)\n",
    "    end = dateutil.parser.parse(end)\n",
    "    days_ = end - start\n",
    "    days_ = days_.days\n",
    "    \n",
    "    # Calculate the number of deaths (height / opposite)\n",
    "    deaths = days_ * (math.tan(math.radians(angle)))\n",
    "    \n",
    "    return int(deaths), days_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_value(number): \n",
    "    \"\"\"Changes number to a readable number with ','s for 000s'\"\"\"\n",
    "    \n",
    "    return (\"{:,}\".format(number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create global_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df = slim_df.groupby('Date_').sum()\n",
    "global_df = global_df[['Confirmed', 'Deaths']]\n",
    "global_df['Mortality_Rate'] = global_df['Deaths'] / global_df['Confirmed']\n",
    "global_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip outliers for mortality\n",
    "global_df = global_df[global_df['Mortality_Rate'] <= .1]\n",
    "global_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally there are data corrections that occur. This causes downstream machine learning challenges with plotting and the ARIMA model. Deal with this here now, by taking the mean of the outliers on those dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Daily Deaths column\n",
    "global_df['Daily_Deaths'] = global_df['Deaths'] - global_df['Deaths'].shift()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Daily_Deaths is negative, we have a problem. Take the average of the before Deaths row and the after Deaths row and use that as a plug value for the Deaths row where the Daily Deaths went negative. \n",
    "\n",
    "We could fix this with a .apply. However, that would loop thru a million plus rows and would take a long time. Lets use vector operations with Pandas.\n",
    "\n",
    "First identify all of the negative instances. Take those rows PLUS the row before and row after and put them in a new temporary dataframe. In temp fix the values. Since python uses pointers this will update the values in the original global_df! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = global_df[global_df['Daily_Deaths'] < 0]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index_list = []\n",
    "for row in temp.itertuples():\n",
    "    row_index = row.Index\n",
    "    row_index_list.append(row_index)\n",
    "row_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(row_index_list)):\n",
    "    \n",
    "    # Get start and end dates of rows of interest\n",
    "    start_date_row = row_index_list[idx] - timedelta(days=1)\n",
    "    end_date_row = row_index_list[idx] + timedelta(days=1)\n",
    "    \n",
    "    # Now that we have the rows, start the adjustments\n",
    "    temp2 = global_df.loc[start_date_row: end_date_row, :]\n",
    "    \n",
    "    # Mean for Confirmed_Cases\n",
    "    temp2.iloc[1,0] = (temp2.iloc[0,0] + temp2.iloc[-1,0]) / 2\n",
    "    \n",
    "    # Mean for Deaths\n",
    "    temp2.iloc[1,1] = (temp2.iloc[0,1] + temp2.iloc[-1,1]) / 2\n",
    "    \n",
    "    # Fix Mortality\n",
    "    temp2.iloc[1,2] = temp2.iloc[1,1] / temp2.iloc[1,0]\n",
    "    \n",
    "    display(temp2)\n",
    "\n",
    "# Don't bother fixing Daily_Deaths. Just drop it.\n",
    "global_df.drop('Daily_Deaths', axis=1, inplace=True) \n",
    "global_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot global mortality rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mortality_rate(global_df, 'Global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mortality rate as of the last day of this dataset\n",
    "print(f'The global mortality rate for confirmed cases is currently\\\n",
    " {round((global_df[\"Mortality_Rate\"].iloc[-1])*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is extremely likely that the number of cases is at least 10 times higher than is being Confirmed. So, this mortality rate is likely 1/10 of the number shown here. Just google this to see all of the papers that are based on this under reporting \"covid confirmed vs not reported cases what are the ratios\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confirmed and Deaths in one plot\n",
    "plot_confirmed_death(global_df, 'Global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run plot for New York\n",
    "new_york = all_df[all_df['Admin2'] == 'New York City'].copy(deep=True)\n",
    "new_york.set_index('Date_', drop=True, inplace=True)\n",
    "plot_confirmed_death(new_york, 'New york')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are both VERY scary plots. They do not show any abatement or signs of flattening. If anything they have gone exponentially higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Who are the best and worst countries dealing with Covid-19?\n",
    "\n",
    "We need a dataframe that has these columns but we ONLY want the last date in the Covid-19 data. Then a groupby of that dataframe. Then to this dataframe we append Deaths_e6, Confirmed_e6,  Median_Income, and the Transparency International score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now merge all_df with country_codes_df.\n",
    "# First of all rename Country in country_codes_df to Country_Region to make the inner join simple\n",
    "covid_pop_inc_df = all_df.merge(country_codes_df)\n",
    "covid_pop_inc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(covid_pop_inc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets put covid_pop_inc_df on a diet\n",
    "covid_pop_inc_df = covid_pop_inc_df[['Date_', 'Alpha_3', 'Country_Region', 'Lat', 'Long_', 'Confirmed', 'Deaths']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_pop_inc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_inc_cc_codes.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_inc_cc_codes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_pop_inc_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_pop_inc_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_df = covid_pop_inc_df.groupby(['Date_','Alpha_3'], as_index=False)['Confirmed', 'Deaths'].sum()\n",
    "end = q1_df.iloc[-1,0].strftime('%Y-%m-%d')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_df = q1_df[q1_df['Date_'] == end]\n",
    "print(q1_df.shape)\n",
    "q1_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to merge the population and median income data with this.\n",
    "q1_df = q1_df.merge(pop_inc_cc_codes)\n",
    "q1_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now convert Confirmed and Deaths to a rate per million population\n",
    "q1_df['Confirmed_e6'] = q1_df['Confirmed'] / (q1_df['Pop2020'] / 1000000)\n",
    "q1_df['Deaths_e6'] = q1_df['Deaths'] / (q1_df['Pop2020'] / 1000000)\n",
    "q1_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of this df\n",
    "q1_df_c = q1_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not think the reporting is correct for countries that are very poor. We are restricting our analysis to just the top 50% of all countries based on MedianPerCapitaIncome AND the country must have a Transparency International rating of 60 or over. The USA has a rating of 69. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_df = q1_df[q1_df['TI_2019'] >= 60]\n",
    "q1_median_income = int(q1_df['MPC_Inc'].median())\n",
    "q1_df = q1_df[q1_df['MPC_Inc'] >= q1_median_income]\n",
    "q1_df.sort_values('Deaths_e6', inplace=True)\n",
    "print(q1_df.shape)\n",
    "q1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mortality_population(q1_df, '>=')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique Alpha_3 codes (Country Codes) do we have in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_df['Alpha_3'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a1'></a>\n",
    "### Answer to Question 1\n",
    "\n",
    "> The above graph shows who is performing the best and the worst on the covid-19 pandemic. The only countries included in this bar graph have a Transparency International score of >= 60 and a MedianPerCapita Income >= 50% of the worlds countries. Countries with the shortest bars are doing better. Remember this IS adjusted for population. So, the fact that New Zealand is right at the top is remarkable. We should be looking at Australia and New Zealand to understand how they achieved this. \n",
    "\n",
    "> The USA is in the bottom third and the UK is at the very bottom. Perhaps a wake up call for their populations? The universe of countries that I chose for this was severely restricted by having the above cutoffs. As a result I feel this is a honest evaluation of the facts.\n",
    "\n",
    "> One sobering thought here is that there are 203 unique Alpha_3 codes in this dataset. That corresponds to 203 countries. We are only looking at 28 of them. This is because of the concern over the honesty of the data. In otherwords the challenge that we are looking at here is only the tip of the iceberg. NB There are 195 countries in the world https://www.thoughtco.com/number-of-countries-in-the-world-1433445. The John Hopkins data has odd Country_Regions such as Cruise Ships. \n",
    "\n",
    "> I also looked at only countries that are \"poor\" but \"honest\" below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets se how the other half lives:)\n",
    "no_money_but_honest = q1_df_c[q1_df_c['TI_2019'] >= 60]\n",
    "no_money_but_honest = no_money_but_honest[no_money_but_honest['MPC_Inc'] < q1_median_income]\n",
    "no_money_but_honest.sort_values('Deaths_e6', inplace=True)\n",
    "\n",
    "# Call the plotting function\n",
    "mortality_population(no_money_but_honest, '<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_money_but_honest['Country_Region'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taiwan is an island and was very aware of the dangers due to its intimate relationship with China. Singapore has great control over its population. Botswana has a national health system, despite being a poor country. Japan is highly sophisticated but ... it is surprising they are on this list as having a lower Median Per Capita Income. They just missed the cutoff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Which countries have flattened or are flattening the curve?\n",
    "\n",
    "We will take our universe of countries from a set that is twice as large as Q1 above and see if they are experiencing a flattening of the curve. The metric will be the last 14 days with a regression line. If the regression line is flat or going down, they are flattening the curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby that creates a sum for each date for each country (Alpha_3).\n",
    "q2_df = covid_pop_inc_df.groupby(['Date_','Alpha_3'], as_index=False)['Confirmed', 'Deaths'].sum()\n",
    "q2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to merge the population and median income data with this.\n",
    "q2_df = q2_df.merge(pop_inc_cc_codes)\n",
    "print(q2_df.shape)\n",
    "q2_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add the column New_Cases\n",
    "# Calculate # of New_Cases/day\n",
    "q2_df['New_Cases'] = q2_df['Confirmed'] - q2_df['Confirmed'].shift(1) \n",
    "print(q2_df.shape)\n",
    "q2_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whoops there are some outliers here.\n",
    "q2_df[q2_df['New_Cases'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 out the offending values. We are just looking for a regression line here.\n",
    "# That is probably the least harmful cleaning alternative.\n",
    "q2_df['New_Cases'] = q2_df['New_Cases'].apply(lambda x: 0 if x < 0 else x)\n",
    "q2_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now convert New_Casess to a rate per million population\n",
    "q2_df['New_Cases_e6'] = q2_df['Confirmed'] / (q2_df['Pop2020'] / 1000000)\n",
    "q2_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_df.sort_values(['Date_', 'Alpha_3'], inplace=True)\n",
    "print(q2_df.shape)\n",
    "q2_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the df that we need for q4. Lets make a copy of it.\n",
    "q4_df = q2_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need the date of the last row in the df\n",
    "end = q2_df.iloc[-1,0]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want the date x days ago\n",
    "days = 14\n",
    "start = end - timedelta(days=days)\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_df = q2_df[q2_df['Date_'] >= start]\n",
    "q2_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not think the reporting is correct. We are restricting our analysis to countries that have a Transparency International rating >= 60. The USA has a rating of 69. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_df = q2_df[q2_df['TI_2019'] >= 60]\n",
    "print(q2_df.shape)\n",
    "q2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list with no duplicates from Country_Region\n",
    "countries = q2_df['Country_Region'].tolist()\n",
    "countries = (set(countries))\n",
    "countries = list(countries)\n",
    "countries.sort()\n",
    "print(len(countries))\n",
    "print(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did not run this one because the output was too small to be of use. \n",
    "# I include it here because if I was reviewing this I would ask why I did not use FacetGrid.\n",
    "\n",
    "# g = sns.FacetGrid(q2_df, col=\"Country_Region\", col_wrap=4, height=2)\n",
    "# g.map(sns.pointplot, 'Date_String', 'New_Cases', order=date_string, color=\".3\", ci=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the below regression plot in full size. In small size (above), you lose the details of whether or not the # of New_Cases is trending up or down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run regression line plot and put countries into the appropriate list.\n",
    "flat = []\n",
    "falling = []\n",
    "rising = []\n",
    "\n",
    "\n",
    "for country in countries:\n",
    "\n",
    "    # Just the country of interest\n",
    "    recent = q2_df[q2_df['Country_Region'] == country]\n",
    "    \n",
    "    # Plot\n",
    "    slope, intercept = regression_new_cases(recent, country, days)\n",
    "    \n",
    "    # Check for flat\n",
    "    if  abs(slope) / intercept < .005:\n",
    "        flat.append(country)\n",
    "    \n",
    "    # Check for falling\n",
    "    elif slope < 0:\n",
    "        falling.append(country)\n",
    "        \n",
    "    # Else rising\n",
    "    else:\n",
    "        rising.append(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('flat\\n', sorted(flat))\n",
    "print('\\nfalling \\n', sorted(falling))\n",
    "print('\\nrising \\n', sorted(rising))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a2'></a>\n",
    "### Answer to Question 2\n",
    "\n",
    "> The above graphs show you the flat, rising, and falling regression lines for each country. The flat list contains the countries that have daily flat numbers of New_Cases of covid-19. The falling list are those countries that have lower numbers of New_Cases over the 14 day period. The rising list are the countries where increased trouble is very close at hand. \n",
    "\n",
    "> We do not cutoff based on Median Per Capita Income on this list. The only cutoff is the Transparency International score. That is why we have ~ twice as many countries being considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Can I see a global geographic representation of infections?\n",
    "\n",
    "Yes, however, we are only going to do this for the last date that we have data on. The John Hopkins dashboard at https://coronavirus.jhu.edu/map.html is an impressive interactive facility that has the ability to visualize data geographically at a much greater level of sophistication than what I will be doing here. The purpose here was to append demographic information (population and Median Per Capita Income) and a \"honesty score\" via the Transparency International score so that we cam gain some confidence about the usefulness of this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby that creates a sum for each date for each country (Alpha_3).\n",
    "q3_df = covid_pop_inc_df.groupby(['Date_','Alpha_3'], as_index=False)['Confirmed', 'Deaths'].sum()\n",
    "q3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to merge the population and median income data with this.\n",
    "q3_df = q3_df.merge(pop_inc_cc_codes)\n",
    "print(q3_df.shape)\n",
    "q3_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict the analysis to just the most recent date for data\n",
    "q3_df = q3_df[q3_df['Date_'] == end]\n",
    "q3_df.sort_values('Country_Region', inplace=True)\n",
    "print(q3_df.shape)\n",
    "q3_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_df.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above looks fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for Deaths Per Million\n",
    "q3_df['Deaths_e6'] = q3_df['Deaths'] / (q3_df['Pop2020'] / 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numeric column names\n",
    "num_cols = q3_df.select_dtypes([np.number]).columns\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a dictionary with the numeric feature columns\n",
    "feature_dict = dict(zip(num_cols, range(len(num_cols))))\n",
    "feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have the plot code in a function, Run all of the numeric columns thru this.\n",
    "# First create a dictionary with the appropriate strings\n",
    "feature_dict.update({num_cols[0]: ['Confirmed', 'Fewest Cases', 'Covid-19 Confirmed Cases By Country'],\n",
    "                    num_cols[1]: ['Deaths', 'Fewest Deaths', 'Covid-19 Deaths By Country'],\n",
    "                    num_cols[2]: ['MPC_Inc', 'Lowest', 'Median Per Capita Income By Country'],\n",
    "                    num_cols[3]: ['Pop2020', 'Lowest', 'Population By Country'],\n",
    "                    num_cols[4]: ['TI_2019', 'Least Honest', 'Transparency International Score By Country'],\n",
    "                    num_cols[5]: ['Deaths_e6', 'Fewest Deaths', 'Covid-19 Deaths By Country Per Million Population']\n",
    "                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare map for all features\n",
    "for feature in num_cols:\n",
    "    plot_world_map(q3_df, feature_dict[feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a3'></a>\n",
    "### Answer to Question 3\n",
    "\n",
    "> The above images quickly tell you where the challenges are. Any country that shows color on the 'Covid-19 Deaths By Country Adjusted For Population' map is in trouble. The grey countries are not reporting data. If you look at the 'Transparency International Score By Country' (right above that one), you can immediately see the issue about reporting. Countries that are honest are reporting and as a result report higher deaths. The vast majority of countries are either not reporting or are \"not honest\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model'></a>\n",
    "# Model Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What is the projected global mortality by December 31, 2020?\n",
    "\n",
    "The way we are going to answer this is to look at a correlation heat map. That will instruct as to what variable would be helpful in terms of predicting deaths. Then we are simply going to do a simple regression line and extend it using trigonometry. Finally I will run an ARMIM model that predicts every day going forward to the end of 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at df just to remind me of what I have.\n",
    "q4_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the correlations. Should be pretty good.\n",
    "sns.heatmap(df.corr(), annot=True, fmt=\".2f\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation is very high between Confirmed and Deaths. There are obvious cases of multicollinearity in the features that are in this dataset (Confirmed, Recovered, Active). There is considerable uncertainty about the quality of the data that is being reported. We  want to predict to the end of 2020 what will be the total number of global deaths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply do not need all of the complexity of the q4_df dataset to answer this question. The global_df that we created earlier in the notebook works quite well for this. It is created via a groupby on Date_ with a sum. This yields the following very simple dataset after dropping a few columns. Remember though, for Confirmed and for Deaths the numbers are cumulative. Mortality_Rate is a simple division based on the previous 2 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ARIMA Model\n",
    "\n",
    "I have a number of features that we could use to predict mortality. However, the obvious one is to simply use Death. This machine learning algorithm is well suited to univariate time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what is going to happen to the USA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and summarize a stationary version of the time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if stationary\n",
    "# Create a Daily Deaths column\n",
    "global_df['Daily_Deaths'] = global_df['Deaths'] - global_df['Deaths'].shift()\n",
    "global_df.dropna(inplace=True)\n",
    "stationary = global_df['Daily_Deaths'].values\n",
    "result = adfuller(stationary)\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot differenced data\n",
    "global_df['Daily_Deaths'].plot()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a little tough to read. Lets create a moving average to smooth the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph a 7 day rolling average rate\n",
    "global_df['moving_average'] = global_df['Daily_Deaths'].rolling(7).mean()\n",
    "global_df.dropna(inplace=True)\n",
    "global_df['moving_average'] = global_df['moving_average'].astype(np.int)\n",
    "title = 'Global Historical 7 Day Moving Average of Deaths for Covid-19'\n",
    "plt.ylabel('Daily_Deaths')\n",
    "plt.title(title)\n",
    "global_df['moving_average'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly a second wave and a VERY large one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the ARIMA model to grid search a solution to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF and PACF plots of the time series\n",
    "series = global_df['Deaths']\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(211)\n",
    "plot_acf(series, ax=plt.gca())\n",
    "plt.subplot(212)\n",
    "plot_pacf(series, ax=plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The ACF shows significant lags to 10 time steps.\n",
    "- The PACF shows significant lags to 2 time step.\n",
    "\n",
    "This quick analysis suggests an ARIMA(10,2,1) on the raw data may be a good starting point. I ran it. It did not converge. So, lets evaluate a series of ARIMA models with a try and except block to avoid \"LinAlgError: SVD did not converge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_arima_model(args):\n",
    "    \"\"\"evaluate an ARIMA model for a given order (p,d,q) and return RMSE and order\"\"\"\n",
    "    \n",
    "    # distribute args to appropriate variables\n",
    "    test, history, order = args\n",
    "    \n",
    "    # make predictions\n",
    "    predictions = []\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=order)\n",
    "        \n",
    "        # fit model\n",
    "        try:\n",
    "            model_fit = model.fit(trend='nc', disp=0)\n",
    "            yhat = model_fit.forecast()[0]\n",
    "            predictions.append(yhat)\n",
    "            history.append(test[t])\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    # calculate out of sample error\n",
    "    try:\n",
    "        rmse = sqrt(mean_squared_error(test, predictions))\n",
    "        print('RMSE is', round(rmse,3), 'with ARIMA of', order)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Model did not fit/predict so unable to compute RMSE for order', order)\n",
    "        rmse = 999999\n",
    "    \n",
    "    return (rmse, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(X, arima_list):\n",
    "    \"\"\"evaluate combinations of p, d and q values for an ARIMA model\"\"\"\n",
    "    \n",
    "    # prepare training dataset\n",
    "    X = X.astype('float32')\n",
    "    train_size = int(len(X) * 0.50)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = list(train)\n",
    "    rmse_list = []\n",
    "    \n",
    "    # Need to create the same number of inputs for each argument into the parallel function\n",
    "    test_list = len(arima_list)*[X]\n",
    "    history_list = len(arima_list)*[history]\n",
    "    zip_list = list(zip(test_list, history_list, arima_list))\n",
    "    \n",
    "    # call function and run in parallel\n",
    "    rmse_list = Parallel(n_jobs=-1, verbose=10)(delayed(evaluate_arima_model)(args) for args in zip_list)\n",
    "                   \n",
    "    # Sort the RMSEs\n",
    "    rmse_list.sort(key=lambda tup: tup[0])\n",
    "    \n",
    "    # Sometimes, we do not have any ARIMA models that successfully fit and predict.\n",
    "    try:\n",
    "        print(f'\\nBest RMSE Score is {round(rmse_list[0][0],3)} with ARIMA of {rmse_list[0][1]}')\n",
    "    except:\n",
    "        print('No ARIMA models fit and predicted successfully. Try different p,d,q parameters')\n",
    "    \n",
    "    return rmse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "series = global_df['Deaths']\n",
    "\n",
    "# evaluate parameters\n",
    "p_values = range(1,8)\n",
    "d_values = range(1,3)\n",
    "q_values = range(0,3)\n",
    "\n",
    "# Generate all different combinations of p, d and q triplets\n",
    "arima_list = list(itertools.product(p_values, d_values, q_values))\n",
    "\n",
    "rmse_list = evaluate_models(series.values, arima_list)\n",
    "best_cfg = rmse_list[0][1]\n",
    "\n",
    "# Create a df to display the rmse and pdq\n",
    "rmse_order_df = pd.DataFrame({'RMSE': [x[0] for x in rmse_list], 'Order': [x[1] for x in rmse_list]})\n",
    "rmse_order_df['RMSE'] = rmse_order_df['RMSE'].astype(np.int)\n",
    "rmse_order_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A RMSE of 999999 means that the fit/predict did not work for ARIMA. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize residual errors for an ARIMA model\n",
    "\n",
    "# load data\n",
    "series = global_df['Deaths']\n",
    "\n",
    "# prepare data\n",
    "X = series.values\n",
    "X = X.astype('float32')\n",
    "train_size = int(len(X) * 0.50)\n",
    "train, test = X[0:train_size], X[train_size:]\n",
    "\n",
    "# walk-forward validation\n",
    "history = [x for x in train]\n",
    "predictions = []\n",
    "for i in range(len(test)):\n",
    "    \n",
    "    # predict\n",
    "    model = ARIMA(history, order=best_cfg)\n",
    "    model_fit = model.fit(trend='nc', disp=0)\n",
    "    yhat = model_fit.forecast()[0]\n",
    "    predictions.append(yhat)\n",
    "    \n",
    "    # observation\n",
    "    obs = test[i]\n",
    "    history.append(obs)\n",
    "\n",
    "# errors\n",
    "residuals = [test[i]-predictions[i] for i in range(len(test))]\n",
    "residuals = pd.DataFrame(residuals)\n",
    "print(residuals.describe())\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "residuals.hist(ax=plt.gca())\n",
    "plt.subplot(212)\n",
    "residuals.plot(kind='kde', ax=plt.gca())\n",
    "plt.show();\n",
    "\n",
    "# Save mean for bias adjustment below\n",
    "bias = residuals.describe()\n",
    "bias = bias.iloc[1][0]\n",
    "print('\\nbias saved in bias for subsequent run is:', bias, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows that the distribution has a right skew and that the mean is non-zero. This is perhaps a sign that the predictions are biased. The distribution of residual errors is also plotted. The graphs suggest a Gaussian-like distribution with a longer left tail. Lets see if the bias made any real difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize residual errors from bias corrected forecasts\n",
    "\n",
    "# load data\n",
    "series = global_df['Deaths']\n",
    "\n",
    "# prepare data\n",
    "X = series.values.astype('float32')\n",
    "train_size = int(len(X) * 0.50)\n",
    "train, test = X[0:train_size], X[train_size:]\n",
    "\n",
    "# walk-forward validation\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "bias = bias # from above run\n",
    "for i in range(len(test)):\n",
    "    \n",
    "    # predict\n",
    "    model = ARIMA(history, order=best_cfg)\n",
    "    model_fit = model.fit(trend='nc', disp=0)\n",
    "    yhat = bias + float(model_fit.forecast()[0])\n",
    "    predictions.append(yhat)\n",
    "    \n",
    "    # observation\n",
    "    obs = test[i]\n",
    "    history.append(obs)\n",
    "\n",
    "# report performance\n",
    "rmse = sqrt(mean_squared_error(test, predictions))\n",
    "print('RMSE: %.3f' % rmse)\n",
    "\n",
    "# summarize residual errors\n",
    "residuals = [test[i]-predictions[i] for i in range(len(test))]\n",
    "residuals = pd.DataFrame(residuals)\n",
    "residuals = residuals.rename({0: 'Residual Statistics'}, axis=1)\n",
    "print(residuals.describe())\n",
    "\n",
    "# plot\n",
    "# histogram\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "residuals.hist(ax=plt.gca())\n",
    "\n",
    "# density\n",
    "plt.subplot(212)\n",
    "residuals.plot(kind='kde', ax=plt.gca())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias had almost no effect on the RMSE. Considering that we will be looking at deaths in the range of 6 to 7 figures, not surprising that this made so little difference. \n",
    "\n",
    "The summary of the forecast residual errors shows that the mean was indeed moved to a value very close to zero. Finally, density plots of the residual error do show a small shift towards zero. Lets take a look at test vs predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble title\n",
    "title = ('Model Prediction of Test vs Predictions For Covid-19 ({}) For {} Days').format('Global', len(test))\n",
    "plt.title(title)\n",
    "\n",
    "# Create x and y axis labels\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Cumulative Deaths By Day')\n",
    "\n",
    "# Create plot\n",
    "plt.plot(test, linewidth=2, label='Test')\n",
    "plt.plot(predictions, linewidth=2, label='Predictions')\n",
    "plt.legend()\n",
    "plt.savefig(r'pics_final/test_vs_prediction.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow hard to believe that this is that accurate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save finalized model to file\n",
    "\n",
    "def __getnewargs__(self):\n",
    "    \"\"\"# monkey patch around bug in ARIMA class\"\"\"\n",
    "    return ((self.endog),(self.k_lags, self.k_diff, self.k_ma))\n",
    "\n",
    "ARIMA.__getnewargs__ = __getnewargs__\n",
    "\n",
    "# load data\n",
    "series = global_df['Deaths']\n",
    "\n",
    "# prepare data\n",
    "X = series.values.astype('float32')\n",
    "\n",
    "# fit model\n",
    "model = ARIMA(X, order=best_cfg)\n",
    "model_fit = model.fit(trend='nc', disp=0)\n",
    "\n",
    "# bias constant, could be calculated from in-sample mean residual\n",
    "bias = bias\n",
    "\n",
    "# save model\n",
    "model_fit.save(r'data/model.pkl')\n",
    "np.save(r'data/model_bias.npy', [bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load finalized model\n",
    "model_fit = ARIMAResults.load(r'data/model.pkl')\n",
    "bias = np.load(r'data/model_bias.npy')\n",
    "\n",
    "# Pick a future to predict. 0 means literally tomorrow.\n",
    "yhat = bias + float(model_fit.forecast()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the float so that it is nicely readable\n",
    "yhat = yhat.astype(int)\n",
    "yhat = place_value(yhat[0])\n",
    "print(f'Current global deaths are: {yhat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This proves the model was saved,  loaded successfully, and performed a prediction. Lets forecast every day between now and March 31, 2020. We are also going to use ALL of the data since we have now put the model into \"production\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "series = global_df['Deaths']\n",
    "X = series.values\n",
    "\n",
    "\n",
    "# fit model\n",
    "model = ARIMA(X, order=(best_cfg))\n",
    "model_fit = model.fit(disp=0)\n",
    "\n",
    "# forecast December 31, 2021\n",
    "start = global_df.index[-1]\n",
    "end = pd.to_datetime('2023-01-01', infer_datetime_format=True)\n",
    "steps = end - start\n",
    "steps = steps.days\n",
    "\n",
    "# Fit the out of sample\n",
    "forecast = model_fit.forecast(steps=steps)[0]\n",
    "\n",
    "# Add the bias\n",
    "predictions = []\n",
    "for yhat in forecast:\n",
    "\n",
    "    yhat = bias[0] + yhat\n",
    "    predictions.append(yhat)\n",
    "    \n",
    "# Create a df for reporting\n",
    "# Create a date_range index\n",
    "start = global_df.index[-1]\n",
    "end = start + timedelta(days=steps-1)\n",
    "\n",
    "# Create the df\n",
    "forecast_df = pd.DataFrame({'Deaths': predictions}, index=pd.date_range(start=start, end=end))\n",
    "\n",
    "# Shift Deaths by one day to make it line up correctly with the date. \n",
    "forecast_df['Deaths'] = forecast_df['Deaths'].shift(1)\n",
    "forecast_df = forecast_df[1:]\n",
    "forecast_df.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a pretty big number. Lets check and see if it is reasonable. We will look at global_df and calculate its daily percent increase. Then we will look at forecast_df and see if it is comparable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a copy of global_df\n",
    "global_df_c = global_df.copy(deep=True)\n",
    "\n",
    "# Drop the extra columns in global_df_c compared to report_df\n",
    "global_df_c = global_df_c[['Deaths']]\n",
    "\n",
    "# Create the extra columns\n",
    "global_df_c['Increase'] = global_df_c['Deaths'] - global_df_c['Deaths'].shift(1)\n",
    "global_df_c['Percent_Increase'] = (global_df_c['Increase'] / global_df_c['Deaths']) * 100\n",
    "global_df_c['Percent_Increase'] = round(global_df_c['Percent_Increase'], 2)\n",
    "global_df_c['Deaths'] = global_df_c['Deaths'].astype(np.int)\n",
    "\n",
    "# Look at it\n",
    "global_df_c.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at daily percent increase for first 5 days\n",
    "forecast_df['Increase'] = forecast_df['Deaths'] - forecast_df['Deaths'].shift(1)\n",
    "forecast_df['Percent_Increase'] = (forecast_df['Increase'] / forecast_df['Deaths']) * 100\n",
    "forecast_df['Percent_Increase'] = round(forecast_df['Percent_Increase'] ,2)\n",
    "forecast_df['Deaths'] = forecast_df['Deaths'].astype(np.int)\n",
    "forecast_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('These means all have to do with the day to day percentage increase.\\n')\n",
    "print(f'Last 14 days actual data mean is: {round(global_df_c.Percent_Increase.iloc[-14:].mean(), 2)}')\n",
    "print(f'First 14 days forecast mean is: {round(forecast_df.Percent_Increase.iloc[:14].mean(), 2)}')\n",
    "print(f'Actual data mean is: {round(global_df_c.Percent_Increase.mean(), 2)}')\n",
    "print(f'Forecast mean is: {round(forecast_df.Percent_Increase.mean(), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If anything the ARIMA model is being MORE conservative than the data would suggest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = place_value(int(forecast_df[\"Deaths\"].iloc[-1]))\n",
    "forecast_date = forecast_df.index[-1].strftime('%Y-%m-%d')\n",
    "print(f'The prediction is for {predicted} cumulative deaths to occur by {forecast_date}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visualize that.\n",
    "predict_deaths(forecast_df, 'Global')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA thinks that this is going to keep on going. I can certainly see why. There is nothing in the dataset that says it will do anything other than what it is doing here, which is heading up and to the right very rapidly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a4'></a>\n",
    "### Answer to Question 4\n",
    "\n",
    "> I created an ARIMA model. This predicts ~ 2 million people will not make it to 2021. These numbers will change if you rerun the notebook with the most recent data. The model was incredibly accurate in its test vs prediction scores. I think this is because the math of a pandemic is so simple. It is easy for the model to learn it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='q5'></a>\n",
    "### Question 5\n",
    "\n",
    "The answer to Q1 above, provides a fair answer of who is doing better or worse managing the covid pandemic by country. However, it is still a little abstract (deaths per million). Lets put this in very personal terms. How many deaths are there being caused by political mismanagement.\n",
    "\n",
    "Lets divide the 14 countries that met the median per capita income cutoff and the transparency international score into 3 parts. The top 3, middle 8, and bottom 3. Lets take the median deaths per million of the top 3 countries and apply it to the bottom 3. Then take a percentage and that gives you what is the percentage contribution of a lack of leadership to the death of a person in that country from covid. Multiply that by deaths and you have the number of unnecessary deaths. You also have a \"percentage blame factor\" called mismanagement. Remember we have controlled for money and honesty. It just comes down to leadership. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refresh our memory what does this df look like for this Q/A\n",
    "q1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy\n",
    "q5_df = q1_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets reset the index. It is a little confusing right now.\n",
    "q5_df.reset_index(drop=True, inplace=True)\n",
    "q5_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "q5_df = q5_df[['Date_', 'Alpha_3', 'Country_Region', 'Deaths', 'Deaths_e6']]\n",
    "q5_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the median for the top 3 (Deaths_e6)\n",
    "top_median = q5_df.iloc[:3,].median()[-1]\n",
    "top_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the bottom 3\n",
    "q5_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the median for the bottom 3 (Deaths_e6)\n",
    "bottom_median = q5_df.iloc[-3:,].median()[-1]\n",
    "bottom_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the ratio between these 2\n",
    "ratio = top_median / bottom_median\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage assignment to political leadership\n",
    "mismanagement = round((1 - ratio),4)\n",
    "print(f'{mismanagement}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5_bottom = q5_df.tail(3)\n",
    "q5_bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deaths due to political mismanagement\n",
    "q5_bottom['Avoidable'] = (q5_bottom['Deaths'] * mismanagement).astype(np.int)\n",
    "q5_bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Buck Stops At The Leadership\\n')\n",
    "for idx in range(3):\n",
    "    print('{} has {} avoidable deaths'.format(\n",
    "        q5_bottom['Country_Region'].iloc[idx],\n",
    "        place_value(q5_bottom['Avoidable'].iloc[idx]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5_bottom.plot.barh(x = 'Country_Region', \n",
    "                    y = ['Deaths', 'Avoidable'],\n",
    "                    title = 'Political Leadership Responsibility',\n",
    "                    legend = 'reverse',\n",
    "                    colormap = 'RdYlGn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a5'></a>\n",
    "### Answer to Question 5\n",
    "\n",
    "If you live in these bottom 3 countries this means if that country had managed the pandemic as well as the median of the top 3, this many people would have lived. 98% of the problem lies with leadership (or lack of)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "# Results and Conclusions\n",
    "\n",
    "> We answered 4 questions in this notebook. Please click on these links to look at those answers.\n",
    "<li style=\"margin-left: 40px\"><a href=\"#a1\">Answer To Q1</a></li>\n",
    "<li style=\"margin-left: 40px\"><a href=\"#a2\">Answer To Q2</a></li>\n",
    "<li style=\"margin-left: 40px\"><a href=\"#a3\">Answer To Q3</a></li>\n",
    "<li style=\"margin-left: 40px\"><a href=\"#a4\">Answer To Q4</a></li>\n",
    "<li style=\"margin-left: 40px\"><a href=\"#a5\">Answer To Q5</a></li>\n",
    "\n",
    "> 24 of 28 countries (that met the cutoffs) have a flattening of the curve presently. That is VERY encouraging.\n",
    "\n",
    "> It is surprising to see rich countries struggle with this virus. There is a saying that democracies get the type of leadership that they deserve. Hopefully this is a wake up call that electing incompetent politicians with massive character flaws to the highest office in the land can and does cause death. This time those deaths are being counted. They are not hidden in Syria, Afghanistan, etc. They are right in the home countries that have weak leadership.\n",
    "\n",
    "> Including the Median Per Capita Income and the Transparency International score was helpful in terms of creating cutoffs that made sense.  \n",
    "\n",
    "> Leadership matters. 100s of thousands of people are dying unnecessarily.\n",
    "\n",
    "> One sobering thought here is that there are 203 unique Alpha_3 codes in this dataset. That corresponds to 203 Country_Regions. We are only looking at 28 of them. This is because of the concern over the honesty of the data. In otherwords the challenge that we are looking at here is only the tip of the iceberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='limitations'></a>\n",
    "# Limitations\n",
    "\n",
    "Generally speaking I believe the reporting on the John Hopkins data is extremely suspect. This is due to political and on the ground issues. Politicians do not want to be accused of doing less than a great job, especially when it is obvious they are doing anything but a great job. Many countries simply do not have the infrastructure, systems, etc. to care for the sick and to report. People are dying and they are not reporting those statistics. Cases are happening and they do not have testing. It is likely that the only reasonable data that you will get and it will be still be grossly under reported in terms of actual cases will be from the western democracies. However, the Transparency International score I have included gives me some confidence for all of the findings in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deploy'></a>\n",
    "# Deploy\n",
    "\n",
    "> Update the John Hopkins, Covid-19 data https://github.com/CSSEGISandData/COVID-19 by refreshing your local copy of the GitHub repository. The ARIMA model has been hyper parameter tuned, stored on disk and is ready to go. There is a forecasting module included for this dataset that can be ran at any time. It remains to be seen whether or not its predictions will be accurate. For additonal information please refer to the ReadMe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "350.047px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
